{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "The data is stored in `3` files.\n",
    "1. `train.csv` numbers `18,304` samples and contains the training data including features and labels. \n",
    "2. `test.csv` numbers `4580` samples and contains the testing feature without labels.\n",
    "3. `sample.csv` number `4580` samples and contains the testing labels without features.\n",
    "\n",
    "Every file has table headers and ends in an empty line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_DIR = r'dataset-in/'   # directory holding the data files\n",
    "# names of files holding dataset\n",
    "DATA_FILENAMES = {\\\n",
    "                  r'trainXy': r'train.csv',\\\n",
    "                  r'testX':   r'test.csv',\\\n",
    "                  r'test_y':  r'sample.csv',\\\n",
    "                 }\n",
    "DELIMITER = r','    # used to separate values in DATA_FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "def putDataFromFiles(srcFilenames, dest):\n",
    "    r'''\n",
    "     Puts the data from the files represented by the source filenames\n",
    "     into the given destination dictionary.\n",
    "\n",
    "     Although this function leaves the keys as abstract, it is expected\n",
    "     that keys represent the type of data (trainXy, testX, test_y)\n",
    "     contained in each file whose name the key maps to.\n",
    "\n",
    "     @param srcFilenames : dict<TKey,str> = dictionary mapping to\n",
    "         filenames containing the data\n",
    "     @param dest : dict<? super TKey,np.array> = dictionary to\n",
    "         which to map the arrays\n",
    "     @return `destDict`\n",
    "     '''\n",
    "    # loop through each mapping to the name of the file\n",
    "    for key, file in srcFilenames.items():\n",
    "        # generate the arrays from the data contained therein\n",
    "        dest[key] = np.genfromtxt(\n",
    "            fr'{DATA_DIR}{file}', delimiter=DELIMITER,\n",
    "            skip_header=True, dtype=np.float64)\n",
    "    # for key, file in srcFilenames.items()\n",
    "    return dest\n",
    "# def putDataFromFiles(srcFilenames, dest)\n",
    "\n",
    "# test with the default DATA_FILENAMES\n",
    "if __name__ == \"__main__\":\n",
    "    data = putDataFromFiles(DATA_FILENAMES, {})"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "Let's count the number of samples as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainXy \t   18304\n",
      "testX   \t    4580\n",
      "test_y  \t    4580\n"
     ]
    }
   ],
   "source": [
    "def countSamples(data, callback):\n",
    "    r'''\n",
    "     Performs the callback on each row of a table of the data types to\n",
    "     arrays.\n",
    "     @param data : dict<str,np.array> = data of which to count samples\n",
    "     @param callback : function(str) = to call on each row\n",
    "     '''\n",
    "    for datatype, array in data.items():\n",
    "        callback(f'{datatype:8}\\t{len(array):8}')\n",
    "    # for datatype, array in data.items()\n",
    "# def countSamples(data, callback)\n",
    "\n",
    "# if main module, print the counts from `putDataFromFiles`\n",
    "if __name__ == \"__main__\":\n",
    "    countSamples(data, print)"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "We can assume that the IDs are succeeding in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainXy': True, 'testX': True, 'test_y': True}\n"
     ]
    }
   ],
   "source": [
    "def isEachArrayIdSorted(data, isEachSorted):\n",
    "    r'''\n",
    "     Returns whether each array in the data is sorted by ID.\n",
    "     @param data : dict<TKey,np.array> = data to check for sorting\n",
    "     @param isEachSorted : dict<? super TKey,bool> = dictionary to\n",
    "         whether each array is sorted\n",
    "     @return `isEachSorted`\n",
    "     '''\n",
    "    # loop through each type of data and array in the data\n",
    "    for datatype, array in data.items():\n",
    "        isEachSorted[datatype] = isArrayIdSorted(array)\n",
    "    return isEachSorted\n",
    "# def isEachArrayIdSorted(data, isEachSorted)\n",
    "\n",
    "def isArrayIdSorted(array):\n",
    "    r'''\n",
    "     Returns whether an array is sorted by ID.\n",
    "     @param array : np.array = array to check for sorting\n",
    "     @return true if each row ID of the array is 1 greater than the\n",
    "     previous;  false otherwise\n",
    "     '''\n",
    "    prev_id = int(array[0, 0])  # ID of the previous row\n",
    "    # for each row\n",
    "    for irow in range(1, array.shape[0]):\n",
    "        curr_id = int(array[irow, 0])   # ID of current row\n",
    "        # if the current row is 1 greater than the previous\n",
    "        if (curr_id != (prev_id + 1)):\n",
    "            # the current row is out of order\n",
    "            return False\n",
    "        # update the previous ID\n",
    "        prev_id = curr_id\n",
    "    # no rows out of order\n",
    "    return True\n",
    "# def isArrayIdSorted(array)\n",
    "\n",
    "# if main module, print whether the data is sorted by ID\n",
    "if __name__ == \"__main__\":\n",
    "    print(isEachArrayIdSorted(data, {}))"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "Let's split `trainXy` into features and labels resembling `testX` and `test_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18304, 12)\n",
      "[(18304, 10), (18304,)]\n"
     ]
    }
   ],
   "source": [
    "def splitFeaturesLabels(dataset, removeIds = False, splitLabels = True):\n",
    "    r'''\n",
    "     Divides the dataset into features and labels.\n",
    "     @param dataset : np.ndarray = the dataset to divide\n",
    "     @param removeIds : bool = whether to remove an initial ID column\n",
    "     @return a tuple containing the feature arrays and label vector\n",
    "     '''\n",
    "    # get the number of rows and columns\n",
    "    (num_rows, num_cols) = dataset.shape\n",
    "    # split the dataset\n",
    "    (_, features, M_label_scalars) = \\\n",
    "        np.split(dataset,\n",
    "                 ((1 if removeIds else 0),\n",
    "                  (num_cols - (1 if splitLabels else 0))\n",
    "                 ), axis=1)\n",
    "    # convert to a vector\n",
    "    if (splitLabels):\n",
    "        labels = M_label_scalars.reshape((num_rows,))\n",
    "    else:\n",
    "        labels = M_label_scalars\n",
    "    # if (splitLabels)\n",
    "    return (features, labels)\n",
    "# end def splitFeaturesLabels(dataset)\n",
    "\n",
    "# if main module\n",
    "if __name__ == \"__main__\":\n",
    "    # print the shape of trainXy before splitting\n",
    "    print(data['trainXy'].shape)\n",
    "    # print the shape of each after splitting\n",
    "    print([x.shape for x in splitFeaturesLabels(data['trainXy'], True)])\n",
    "# if __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "Altogether we have read and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18304, 10), (18304,), (4580, 10), (4580,)]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # read in the data from the data files\n",
    "    data = putDataFromFiles(DATA_FILENAMES, {})\n",
    "    # split the training data\n",
    "    (trainX, train_y) = splitFeaturesLabels(data['trainXy'], True)\n",
    "    # get the testing data\n",
    "    (testX, _) = splitFeaturesLabels(data['testX'], True, splitLabels=False)\n",
    "    (_, test_y) = splitFeaturesLabels(data['test_y'], True)\n",
    "    return (trainX, train_y, testX, test_y)\n",
    "\n",
    "# if main module, print the shape of each type of data\n",
    "if __name__ == \"__main__\":\n",
    "    # print the shape of each after splitting\n",
    "    print([x.shape for x in main()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
